
import requests
import concurrent.futures
import os
import cloudscraper
import threading
import hashlib
from urllib.parse import urlparse

# Danh sÃ¡ch API HTTP proxy (VIP + Free, táº­p trung elite/anonymous)
APIS_HTTP = [
    # Proxyscrape (elite proxies)
    "https://api.proxyscrape.com/v4/free-proxy-list/get?request=display_proxies&protocol=http&proxy_format=ipport&format=text&timeout=1000&anonymity=elite",
    # Open Proxy Space
    "https://openproxy.space/list/http",
    # Geonode (lá»c elite, cáº­p nháº­t nhanh)
    "https://proxylist.geonode.com/api/proxy-list?limit=500&page=1&sort_by=lastChecked&sort_type=desc&protocols=http&anonymityLevel=elite",
    # Proxy-List.download
    "https://www.proxy-list.download/api/v1/get?type=http&anon=elite",
    # VSIS.net (Viá»‡t Nam, Æ°u tiÃªn tá»‘c Ä‘á»™)
    "http://36.50.134.20:3000/download/vn.txt",
]

OUT_FILE = "http_vip.txt"
lock = threading.Lock()

# Biáº¿n toÃ n cá»¥c Ä‘á»ƒ Ä‘áº¿m sá»‘ proxy Ä‘Ã£ check
checked_count = 0
checked_lock = threading.Lock()

# HÃ m kiá»ƒm tra tÃ­nh duy nháº¥t cá»§a proxy
def hash_proxy(proxy):
    return hashlib.md5(proxy.encode()).hexdigest()

# B1: Check proxy sá»‘ng (httpbin.org, timeout tháº¥p hÆ¡n)
def check_alive(proxy):
    global checked_count
    test_url = "https://httpbin.org/ip"
    proxies = {"http": f"http://{proxy}", "https": f"http://{proxy}"}
    try:
        r = requests.get(test_url, proxies=proxies, timeout=1.5)  # Giáº£m timeout
        if r.status_code == 200:
            with checked_lock:
                checked_count += 1
                print(f"ğŸ” ÄÃ£ check {checked_count} proxy sá»‘ng", end="\r")
            return proxy
        return None
    except:
        with checked_lock:
            checked_count += 1
            print(f"ğŸ” ÄÃ£ check {checked_count} proxy sá»‘ng", end="\r")
        return None

# B2: Check qua Cloudflare (cdn-cgi/trace) + lÆ°u dáº¡ng IP:Port
def check_cloudflare(proxy):
    global checked_count
    test_url = "https://www.cloudflare.com/cdn-cgi/trace"
    proxies = {"http": f"http://{proxy}", "https": f"http://{proxy}"}
    try:
        scraper = cloudscraper.create_scraper()
        r = scraper.get(test_url, proxies=proxies, timeout=3)  # Giáº£m timeout
        if r.status_code == 200:
            with lock:
                with open(OUT_FILE, "a") as f:
                    f.write(f"{proxy}\n")  # Chá»‰ lÆ°u IP:Port
                print(f"âœ… LÆ°u proxy: {proxy}")
            with checked_lock:
                checked_count += 1
                print(f"ğŸŒ ÄÃ£ check {checked_count} proxy Cloudflare", end="\r")
            return proxy
        return None
    except:
        with checked_lock:
            checked_count += 1
            print(f"ğŸŒ ÄÃ£ check {checked_count} proxy Cloudflare", end="\r")
        return None

# Láº¥y proxy tá»« API
def fetch_api(url):
    proxies = set()  # DÃ¹ng set Ä‘á»ƒ trÃ¡nh trÃ¹ng
    try:
        if "geonode.com" in url:
            r = requests.get(url, timeout=8).json()
            for p in r.get("data", []):
                ip, port = p.get("ip"), p.get("port")
                if ip and port:
                    proxy = f"{ip}:{port}"
                    if hash_proxy(proxy) not in proxies:
                        proxies.add(proxy)
        else:
            r = requests.get(url, timeout=8)
            for line in r.text.splitlines():
                if ":" in line:
                    proxy = line.strip()
                    if hash_proxy(proxy) not in proxies:
                        proxies.add(proxy)
    except Exception as e:
        print(f"âš ï¸ Lá»—i khi láº¥y tá»« {urlparse(url).netloc}: {e}")
    return list(proxies)

def main():
    global checked_count
    
    # XoÃ¡ file cÅ©
    if os.path.exists(OUT_FILE):
        os.remove(OUT_FILE)
        print(f"ğŸ—‘ï¸ ÄÃ£ xoÃ¡ file cÅ©: {OUT_FILE}")

    # Táº£i proxy
    all_proxies = []
    print("ğŸŒ Äang Ä‘Ã o HTTP proxy siÃªu VIP...")
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        results = executor.map(fetch_api, APIS_HTTP)
        for proxies in results:
            all_proxies.extend(proxies)

    all_proxies = list(set(all_proxies))  # Lá»c trÃ¹ng láº§n ná»¯a
    print(f"ğŸ” Tá»•ng proxy HTTP láº¥y Ä‘Æ°á»£c: {len(all_proxies)}")

    # B1: Check proxy sá»‘ng
    print("âš¡ Äang check proxy sá»‘ng...")
    checked_count = 0
    with concurrent.futures.ThreadPoolExecutor(max_workers=150) as executor:
        alive_results = list(executor.map(check_alive, all_proxies))
    print()
    alive = [p for p in alive_results if p]
    print(f"âœ… Proxy sá»‘ng: {len(alive)}/{len(all_proxies)}")

    # B2: Check Cloudflare + lÆ°u ngay
    print("ğŸŒ Äang check proxy qua Cloudflare...")
    checked_count = 0
    with concurrent.futures.ThreadPoolExecutor(max_workers=150) as executor:
        list(executor.map(check_cloudflare, alive))
    print()

    # BÃ¡o cÃ¡o cuá»‘i
    with open(OUT_FILE, "r") as f:
        saved = f.read().splitlines()
    print(f"\nğŸ¯ HoÃ n táº¥t: {len(saved)} proxy siÃªu VIP vÆ°á»£t Cloudflare Ä‘Ã£ lÆ°u vÃ o {OUT_FILE}")

if __name__ == "__main__":
    main()